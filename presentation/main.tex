% Quick start guide
\documentclass[10pt]{beamer}

\usepackage{mathtools}

% Title page details
\title{Stochastic Calculus}
\author{Kellen Kanarios}
\institute{University of Michigan}
\date{\today}

\begin{document}

\begin{frame}
% Print the title page as the first slide
\titlepage
\end{frame}

\begin{frame}[t]
  \frametitle{Background}
  \begin{onlyenv}<1-2>
    \begin{definition}
      A real-valued random variable $X$ is a mapping $X : \Omega \to \mathbb{R}$. Where $\Omega$ is the \textbf{sample space} and $\mathbb{P}$ is the measure of the \textbf{probability space}, such that $\mathbb{P}(\Omega) = 1$.
    \end{definition}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{block}{Definition translated into english}
      \begin{enumerate}
        \item 
          Unbeknownst to us, someone chooses a random $\omega \in \Omega$. Then we see the $X(\omega) \in \mathbb{R}$.
        \item 
          We cannot see the corresponding $\omega \in \Omega$, but the $X(\omega) \in \mathbb{R}$ gives us partial information about $\omega$.
        \item $\mathbb{P}$ tells us how likely subsets $A \subseteq \omega$ are to occur.
      \end{enumerate}
    \end{block}
  \end{onlyenv}
  \begin{onlyenv}<3>
    \begin{example}
      Consider the case where you flip a coin. Using our previous definition, this could be described as $\Omega = \{\text{heads}, \text{tails}\}$ and \\
      \vskip 5pt
      $X(\omega) = \begin{cases}
        1, &\text{ if }\omega = \text{heads}\\
        -1, &\text{ if }\omega = \text{tails}\\
      \end{cases}$ where $\omega \in \Omega$. \\
      \vskip 5pt
      This would yield the familiar notation of $\mathbb{P}(X = 1) = .5$ and $\mathbb{P}(X = -1) = .5$ for a fair coin.
    \end{example} 
    \end{onlyenv}
    \begin{onlyenv}<4->
      \begin{definition}
        A \textbf{stochastic process} is a family of random variables indexed by a time parameter $t \geq 0$.
      \end{definition}
    \end{onlyenv}
    \begin{onlyenv}<5->
      \begin{example}
        A sequence of coin flips. At each time $t$ your process corresponds to a random variable (aka coinflip) $X_t$.
      \end{example}
    \end{onlyenv}
\end{frame}


\begin{frame}[t]
  \frametitle{Wiener Process}
  \begin{onlyenv}<1-5>
  \begin{definition}
    A stochastic process $W$ is called a \textbf{Wiener process} if the follow conditions hold
    \begin{enumerate}
      \item<2-> $W_0 = 0$
      \item<3-> The process $W$ has independent increments
      \item<4-> For $s < t$ the random variable $W_t - W_s$ has the Gaussian distribution $\mathcal{N}(0,t-s)$
      \item<5-> $W$ has continuous trajectories
    \end{enumerate}
  \end{definition}
  \end{onlyenv}
  \begin{onlyenv}<6->
    \begin{theorem}
      A Wiener trajectory is with probability one, nowhere differentiable, and it has locally infinite total variation.
    \end{theorem}
    \begin{figure}
      \includegraphics[scale=.45]{./graphics/wiener-trajectory.png}
      \caption{Wiener trajectory}
    \end{figure}
  \end{onlyenv}
\end{frame}

\begin{frame}[t]
  \frametitle{Quadratic Variation}
  \begin{onlyenv}<1->
    \begin{definition}
      Let $X$ be a stochastic process. Suppose $P$ is a partition of $[0,t]$ denoted $t_k$ and let $\lVert P \rVert$ be the mesh of the partition then the \textbf{quadratic variation} of $X$ on $P$ is defined to be:
          \begin{align*}
            [X]_t = \lim\limits_{\lVert P \rVert \to 0} \displaystyle\sum_{k = 1}^{n}(X_{t_k} - X_{t_{k-1}})^2
          \end{align*}
    \end{definition}
  \end{onlyenv}
  \begin{onlyenv}<2->
    \begin{block}{Comments}
      Note that Quadratic Variation itself is a stochastic process. An intuitive way to think about quadratic variation is the internal clock of a process, describing how randomness accumulates over time.
    \end{block}
  \end{onlyenv}
\end{frame}

\begin{frame}[t]
  \frametitle{Quadratic Variation of Wiener Process}
    \begin{onlyenv}<1-3>
    \begin{theorem}
      Quadratic variation of a Wiener Process on the interval $[0,t]$ is $t$.
    \end{theorem}
    \end{onlyenv}
    \begin{onlyenv}<2-3>
      \begin{block}{Proof.}
        Let $P = \{0 = t_0 \leq t_1 \leq \dots \leq t_m = t\}$ be a partition of the interval $[0,t]$. Then the quadratic variation on $P$ is
        \begin{align*}
          [W]^{P} &= \displaystyle\sum_{k = 1}^{m}(W_{t_k} - W_{t_{k-1}})^2 
        \end{align*}
        \onslide<3->{
          Therefore,
          \begin{align*}
            E\left[[W]^{P}\right] &= \displaystyle\sum_{k = 1}^{m}E\left[(W_{t_k} - W_{t_{k-1}})^2\right]
          \end{align*}
        }
      \end{block}
    \end{onlyenv}
    \begin{onlyenv}<4-8>
      \begin{block}{Stats break}
        \onslide<5->{
        It is a well known fact that 
        \begin{align*}
          Var(X) = E(X^2) - (E(X))^2
        \end{align*}
        }
        \onslide<6->{
        Since $W_{t_{k+1}} - W_{t_{k}} \in \mathcal{N}(0, t_{k+1} - t_{k})$, 
        \begin{align*}
          E(W_{t_{i+1}} - W_{t_{i}}) = 0
        \end{align*}
        }
        \onslide<7->{ 
        It follows that 
        \begin{align*}
          (E(W_{t_{k+1}} - W_{t_{k}})^2) = 0 
        \end{align*}
        }
        \onslide<8->{
        Thus, $Var(W_{t_{k+1}} - W_{t_{k}}) = E((W_{t_{k+1}} - W_{t_{k}})^2)$.
        }
      \end{block}
    \end{onlyenv}
    \begin{onlyenv}<9-11>
      \begin{proof}
      Since $E[(W_{t_{i+1}} - W_{t_{i}})^2] = Var\left[W_{t_{i+1}} - W_{t_{i}}]$, we can write:
      \begin{align*}
        E\left[[W]^{P}\right] &= \displaystyle\sum_{k = 1}^{m}Var\left[W_{t_k} - W_{t_{k-1}}\right]
      \end{align*}
      \onslide<10->{
        Using $W_{t_k} - W_{t_{k-1}} \in \mathcal{N}(0, t_k - t_{k-1})$:
        \begin{align*}
          E\left[[W]^{P}\right] &= \displaystyle\sum_{k = 1}^{m}(t_{k} - t_{k-1}) \\
          &= t
        \end{align*}
      }
      \onslide<11->{ 
        Finally, from the definition of discrete expectation we have that $E[[W]^{P}] \coloneqq \lim\limits_{\lVert P \rVert \to 0} [W]^{P} \coloneqq t$
      }
      \end{proof}
    \end{onlyenv}
    \begin{onlyenv}<12->
      \begin{block}{Implications}
        This motivates us to write 
        \begin{align*}
          \int\limits_{0}^{t}(dW_t)^2 = t
        \end{align*}
        Or equivalently,
        \begin{align*}
          (dW_t)^2 = dt
        \end{align*}
      \end{block}
  \end{onlyenv}
\end{frame}

\begin{frame}[t]
  \frametitle{The Stochastic Integral}
  \begin{onlyenv}<1-3>
    \begin{block}{The Problem}
      Integrals of the form $\displaystyle\int_{0}^{t}g_s dW_s$.
    \end{block}
  \end{onlyenv}
  \begin{onlyenv}<2-3>
    \begin{block}{Possible Solution 1}
      Try to define like the Riemman Integral
      \begin{enumerate}
        \item<2-> $\displaystyle\sum_{k = 1}^{n}g_s(W_{t_{k+1}} - W_{t_{k}})$
        \item<3-> Not possible due to locally unbounded variation of a Wiener Process.
      \end{enumerate}
    \end{block}
  \end{onlyenv}
  \begin{onlyenv}<4->
    \begin{definition}
      A function is \textbf{simple} on $[a,b]$ when there exists deterministic points in time $a = t_0 < t_1 < \dots < t_n = b$ such that $g_s = g_{t_k}$ for all $s \in [t_k, t_{k+1}]$.
    \end{definition}
  \end{onlyenv}
  \begin{onlyenv}<5->
      \begin{block}{Real Solution}
        Now we can define the stochastic integral by
        \begin{align*}
          \displaystyle\int_{a}^{b}g_s dW_s = \displaystyle\sum_{k = 0}^{n - 1}g_{t_k}[W_{t_{k+1}} - W_{t_{k}}]
        \end{align*}
        For some simple function $g_s$.
      \end{block}
  \end{onlyenv}
\end{frame}

\begin{frame}[t]
  \frametitle{Geometric Brownian Motion Model}
  \begin{onlyenv}<1-2>
    \begin{definition}
      \textbf{Geometric Brownian Motion} is a stochastic process whose dynamics follow the stochastic differential equation 
      \begin{align*}
        dX_t = \alpha X_t dt + \sigma X_t dW_t \text{ for some } \alpha,\beta \in \mathbb{R}
      \end{align*}
      Where $dW_t$ is the infinitesimal increment of the Wiener process.
    \end{definition}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{block}{Claim:}
      The closed form of Geometric Brownian Motion is:
      \begin{align*}
        X_t = X_0 e^{(\mu - \frac{\sigma^2}{2})dt + \sigma dW_t}
      \end{align*}
    \end{block}
  \end{onlyenv}
  \begin{onlyenv}<3-6>
    \begin{block}{Proof.}
      Assume $X_t$ follows a Geometric Brownian Motion. Using Ito's lemma,
      \begin{align*}
        d\ln(X_t) = \frac{1}{X_t}dX_t - \frac{1}{2X_t^2}(dX_t)^2
      \end{align*}
      \onslide<4->{
        From our definition of Geometric Brownian Motion,
        \begin{align*}
          d\ln(X_t) = \frac{1}{X_t}(\mu X_t dt + \sigma X_t dW_t) - \frac{1}{2X_t^2}(\mu X_t dt + \sigma X_t dW_t)^2 
        \end{align*}
      }
      \onslide<5->{
        Note that:
        \begin{align*}
          (\mu X_t dt + \sigma X_t dW_t)^2 &= X_t^2(\mu^2dt^2 + \sigma^2dW_t^2 + 2\mu \sigma dtdW_t)
        \end{align*}
      }
  % \end{onlyenv}
      \onslide<6->{
        From Ito's multiplication table, $(dt)^2 = 0 = (dt)(dW_t)$ and $(dW_t)^2 = dt$ such that
        \begin{align*}
          (\mu X_t dt + \sigma X_t dW_t)^2 &= X_t^2 \sigma^2 dt
        \end{align*}
      }
    \end{block}
  \end{onlyenv}
  \begin{onlyenv}<7->
    \begin{proof}
        Substituting back,
        \begin{align*}
          d\ln(X_t) &= (\mu dt + \sigma dW_t) - \frac{1}{2X_t^2}(X_t^2 \sigma^2 dt) \\
          &= (\mu - \frac{\sigma^2}{2})dt + \sigma dW_t.
        \end{align*}
        \onslide<8->{ 
        It follows that $X_t = e^{(\mu - \frac{\sigma^2}{2})dt + \sigma dW_t}$
        }
    \end{proof}
  \end{onlyenv}
\end{frame}

\end{document}
